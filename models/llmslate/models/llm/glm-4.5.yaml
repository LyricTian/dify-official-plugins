model: glm-4.5
label:
  zh_Hans: GLM-4.5
  en_US: GLM-4.5
model_type: llm
features:
- agent-thought
- tool-call
- stream-tool-call
model_properties:
  mode: chat
  context_size: 128000
parameter_rules:
- name: temperature
  use_template: temperature
- name: max_tokens
  use_template: max_tokens
  type: int
  default: 96000
  min: 1
  max: 128000
  help:
    zh_Hans: 指定生成结果长度的上限。如果生成结果截断，可以调大该参数。
    en_US: Specifies the upper limit on the length of generated results. If the generated
      results are truncated, you can increase this parameter.
- name: top_p
  use_template: top_p
- name: top_k
  label:
    zh_Hans: 取样数量
    en_US: Top k
  type: int
  help:
    zh_Hans: 仅从每个后续标记的前 K 个选项中采样。
    en_US: Only sample from the top K options for each subsequent token.
  required: false
- name: frequency_penalty
  use_template: frequency_penalty
- name: response_format
  label:
    zh_Hans: 回复格式
    en_US: Response Format
  type: string
  help:
    zh_Hans: 指定模型必须输出的格式
    en_US: Specifying the format that the model must output
  required: false
  options:
  - text
  - json_object
pricing:
  input: '3.6'
  output: '16.8'
  unit: '0.000001'
  currency: RMB
  input_cached: '0.72'
  billing_type: CONTEXT
  context_prices:
  - input_price: 4800000
    output_price: 19200000
    gt_context_window: 32000
    input_cached_price: 960000
  - input_price: 3600000
    output_price: 16800000
    gt_context_window: 0
    input_cached_price: 720000
